## 리뷰

---

### 오늘 할 일

- 강의
- 직무 meet-up
- 팀 활동 크롤링
    - 정비소 info 데이터
    - 정비소 리뷰 데이터

### 팀 활동 피드백 및 정리

1. 비용·효율 균형

- 최소 비용·최대 효율을 동시에 달성하는 것은 현실적으로 어렵다.
- 둘 중 하나를 고정 변수로 설정하고, 나머지를 극소·극대화하는 방식이 현실적.

2. 의사결정 기준 차이 문제

- 구성원별로 의사결정 기준이 다를 때, 일관된 처리 방식 필요.
- 쿠팡 위너 시스템 예시: 같은 상품을 여러 명이 올리더라도, 최종적으로 하나의 제품만 노출.
- 핵심 원칙: 사용자가 고민해야 하는 요소를 최소화하는 방향.

3. 비교 사례

- 카닥: 동일 문제를 해결하지 못해, 사용자가 여러 리스트를 보고 선택해야 함.
- 보완 방법: 점수제 도입 가능. 단, 설명 가능성 확보가 필요.

4. 사용자 경험 우선

- 목표 기능을 먼저 완성하고, 이후 견적 세부 항목 등을 확장.
- 핵심은 사용자 편의성 극대화.

5. 정비소 데이터 수집

- 카닥은 데이터량은 적지만 리뷰·평점 정보 보유.
- 프로토타입 제작 단계에서는 카닥 크롤링을 우선 적용하기로 결정.

### 직무 밋업

**본부 소개**

**일하는 방식 & 문화**

- **핵심 가치**: 긍정적 마인드, 소통·협업, 책임감, 집요함, 전문성, 새로운 도전·시도, 민첩한 실행
- **목표 지향**: 고객 최우선, 고객 만족을 위해 노력
- **조직 문화**: 수평적 분위기 → 개인의 책임감이 크고 주도적으로 일함

**업무 범위**

- IT 거버넌스 수립
- ICT 담당: Customer channel 영역 담당
- 스마트 팩토리 및 기타 SI(System Integration) 업무: 오토에버에서 수행

**기술 지향성**

- 회사 차원에서 신기술에 대한 관심이 높음
- 신기술을 활용한 변화·혁신에 적극적인 인재 선호

**1. 직무 소개 & 업무 내용**

- **주요 역할**
    - 외부 데이터 소스 수집 및 데이터 파이프라인 구축·운영 (ODS → DW → DM)
    - 데이터 플랫폼 설계·구축
    - 수집 데이터 기반 서비스 제공
    - 지표 수치화·리포트·대시보드 제작
- **데이터 흐름 예시**
    - 차량 → 센서 → 실시간 데이터(바이너리) → 파싱 → 카프카 수집 → 하둡 적재·정규화
    - 개인정보는 가명화 필수
    - 하루 약 350TB 데이터 발생
    - 해외 데이터는 보안 문제로 국내 카프카 서버를 거쳐 통합
- **스케줄링 & 배치**: 전부 Airflow 사용
- **공통 기술 스택**: Airflow, Spark, Hadoop, Scala
    - 시각화: Tabula, Trino
    - IDE/도구: IntelliJ, PyCharm, k9s, OpenLens 등

---

2. **부트캠프와 실무 연계**

- 부트캠프에서 배운 대부분 내용 실무 적용
- AWS 대신 사내 온프레미스 환경 사용
- 배운 기술 스택의 **원리까지 깊게** 이해해 두는 것이 중요 (실무에서 복습 시간 없음)

---

3. **조직·협업 구조**

- CDO 산하: 엔지니어, 사이언티스트, 기획자 등 다양한 직군
- 사이언티스트 요청 시 해당 데이터 수집·제공
- 협업: 데일리 스크럼, 회의 전 Ground Rule 설정
- 법인과 데이터 규제 이슈 소통 (원천 삭제 시 가공 데이터도 삭제)
- 코드 표준화 규칙 유지 (개인·팀별 프로젝트 스타일 차이 방지)

---

4. **실무 팁 & 주의사항**

- **개인정보 처리**: 원천 데이터 수집 → 가명화 → 적재 → 일정 기간 후 원천 삭제
- **데이터 수집 시 주의**: 원천 서버 부하 금지
    
    → 위험 작업(delete/drop) 시 크로스 체크 필수 (코드 리뷰·깃 커밋 리뷰어 지정)
    
- 문서화 중요: 기술 스택·프로세스·기능 명세 등 기록
- 모니터링·알람 시스템 필수 (무분별한 알람은 지양)

---

5. **프로젝트 진행 방식**

- 설계 뼈대(파이프라인 구조) 먼저 구축
- 최종 프로젝트:
    - IaC(Infrastructure as Code)로 인프라 관리
    - AWS 관리 경험 발표
    - LLM 비중 낮추고 파이프라인·개발에 집중
- 시각화는 React 기반 팀도 존재
- 웹/앱으로 배포할 경우 그 이유가 존재해야 함
- 사이언티스트 관점 강조하는 팀도 있음

---

6. **면접 관련**

- 자주 묻는 것:
    - 프로젝트 관련 상세 질의
    - 기술 스택 선택 이유
    - 사고 경험 및 대처 방법
    - 개인정보 처리 방법
- 피해야 할 표현:
    - “대용량 데이터 다뤄보고 싶다” 같은 진부한 멘트
- 좋은 인상:
    - 차별화된 경험·기술
    - 빠른 학습 능력·적극성
    - 새로운 오픈소스 실무 적용 능력
- 준비 추천:
    - GitLab vs GitHub 차이와 사용법
    - Kubernetes(k8s) 및 관리 툴(k9s 등)
    - Airflow 심화, 오픈소스 커밋 경험

---

7. **신입/인턴 기대 역량**

- 적극적 태도와 빠른 학습 속도
- 기초 전공지식 탄탄 (특히 분산 처리 이해)
- 새로운 기술 빠르게 습득 후 적용

---

8. **도메인 & 관심 기술**

- 데이터 수집·전달 자동화 데이터 플랫폼
- 관심 기술:
    - Airflow 3.0.0
    - 분리된 파이프라인 통합
    - Hadoop·Spark 내부 구조
    - Kubernetes

## 회고

---

### Keep

### Problem

### Try

최종 프로젝트에 airflow를 사용하여 task 스케줄 관리를 할 필요가 있어 개인적으로 공부하려고 한다.
