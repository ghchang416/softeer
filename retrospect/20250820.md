## 리뷰

---

## 진행 상황

### 1) 파이프라인 전략

- 우선순위 확정: 공임비 매핑은 뒤로 미루고, 수집→저장→파싱 자동화 파이프라인 먼저 완성.
- 모듈 구조: `fetcher`(수집) / `parser`(파싱) 코드 분리 유지. 초기엔 로컬 경로에 저장하도록 구현.

### 2) Airflow 연동 (로컬 기준)

- 이미지: 시중의 Airflow+Spark 제공 이미지로 빠른 프로토타입 구성(추후 커스텀 빌드 예정).
- DAG: `fetcher → parser` 의존 관계로 태스크 구성, 로컬 디렉토리를 I/O로 사용.
    - 예: `fetcher_task` 완료 시 `data/raw/...`에 HTML 저장 → `parser_task`가 동일 경로 읽어 CSV/중간산출 생성.
- 결과: 공임표 크롤러와 부품몰 크롤러 각각 성공적으로 동작 확인(스케줄은 수동/온디맨드 실행 기준).

### 3) S3 호환 로컬 환경 검증(MinIO)

- 목적: AWS 전환 전, S3 API 호환성/권한 흐름을 로컬에서 검증.
- 구성: MinIO 컨테이너 띄우고, AWS SDK와 동일한 자격/엔드포인트 형식 사용(버킷, 키, prefix 동일 패턴).
- 변경점: 파이프라인의 저장 경로를 로컬 디스크 → MinIO 버킷으로 변경.
- 결과: 업로드/리스트/읽기 정상. Airflow에서 S3Hook/클라이언트로도 접근 확인.

### 4) 실제 AWS S3 전환

- 동일 인터페이스로 엔드포인트만 S3로 전환.
- 결과: 업로드 성공. 버킷/프리픽스 구조 예:
    - `s3://<bucket>/raw/parts/<카테고리>/page_*.html`
    - `meta: source-url, category, page` 등 부가 메타 부여.

### 5) Spark 기반 파싱 도입

- 목표: Airflow `SparkSubmitOperator`로 worker → spark-master(client 모드) 제출.
- 적용 범위: *fetcher는 그대로*, `yaml/dag/parser`만 스파크용으로 수정 예정.

## 내일 할 일

- api server 구축
- 코드 리팩토링 및 로컬 코드 aws 옮기기
- 정비소 추천 시스템 도입 생각
- 공임비 매핑 알고리즘 고안

## 회고

---

### Keep

같은 일을 함께 붙잡고 고민하기보다, 오전 데일리에서 역할과 과제를 명확히 나눠 하루 책임 수행하니 업무 처리 속도가 확실히 빨라졌다.

### Problem

### Try
